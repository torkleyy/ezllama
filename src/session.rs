use crate::error::{Error, Result};
use llama_cpp_2::context::LlamaContext;
use llama_cpp_2::llama_batch::LlamaBatch;
use llama_cpp_2::model::{AddBos, Special};
use llama_cpp_2::sampling::LlamaSampler;
use std::iter::Iterator;
use tracing::{debug, error, info, trace};

use crate::model::Model;

/// A stream of tokens generated by the model
pub struct TokenStream<'s, 'a> {
    /// Reference to the text session
    session: &'s mut TextSession<'a>,
    /// Number of tokens decoded so far
    n_decode: i32,
    /// Current batch for token generation
    batch: LlamaBatch,
    /// Whether generation is complete
    is_complete: bool,
}

impl<'s, 'a> TokenStream<'s, 'a> {
    pub fn join(self) -> String {
        self.collect::<String>()
    }

    /// Get the number of tokens decoded so far
    pub(crate) fn n_decode(&self) -> i32 {
        self.n_decode
    }
}

impl<'s, 'a> Iterator for TokenStream<'s, 'a> {
    type Item = String;

    fn next(&mut self) -> Option<Self::Item> {
        if self.is_complete {
            return None;
        }

        match slide_cache(&mut self.session.ctx, 1) {
            Ok(_) => {}
            Err(e) => {
                error!("Failed to slide cache: {}", e);
                self.is_complete = true;
                return None;
            }
        }

        // Sample the next token
        let token = self.session.sampler.sample(&self.session.ctx, -1);
        self.session.sampler.accept(token);

        // Check for end of generation token
        if self.session.model.model.is_eog_token(token) {
            trace!("End of generation token detected: {token}");
            self.is_complete = true;
            return None;
        }

        // Convert token to text
        let token_text = match self
            .session
            .model
            .model
            .token_to_str(token, Special::Tokenize)
        {
            Ok(text) => {
                trace!(name: "token-gen", "Generated token: {}", text);
                text
            }
            Err(e) => {
                error!("Failed to convert token to string: {}", e);
                self.is_complete = true;
                return None;
            }
        };

        // Create a new batch with the generated token
        match LlamaBatch::get_one(&[token]) {
            Ok(new_batch) => {
                self.batch = new_batch;
            }
            Err(e) => {
                error!("Failed to create batch from generated token: {}", e);
                self.is_complete = true;
                return None;
            }
        }

        // TODO: should we decode eog?
        if let Err(e) = self.session.ctx.decode(&mut self.batch) {
            error!("llama_decode() failed: {}", e);
            self.is_complete = true;
            return None;
        }

        self.n_decode += 1;

        Some(token_text)
    }
}

/// A session for text completion
pub struct TextSession<'a> {
    /// The current prompt
    pub prompt: String,
    /// Reference to the model
    pub(crate) model: &'a Model,
    /// The llama context for this session
    pub(crate) ctx: LlamaContext<'a>,
    /// The llama sampler for this session
    pub(crate) sampler: LlamaSampler,
}

impl<'a> TextSession<'a> {
    /// Create a new text session with context
    pub(crate) fn new_with_context(model: &'a Model, ctx: LlamaContext<'a>) -> Self {
        Self {
            prompt: String::new(),
            model,
            ctx,
            sampler: llama_cpp_2::sampling::LlamaSampler::chain_simple([
                llama_cpp_2::sampling::LlamaSampler::dist(1234),
                llama_cpp_2::sampling::LlamaSampler::greedy(),
            ]),
        }
    }

    /// Add to the prompt and generate tokens
    pub fn prompt(&mut self, prompt: &str) -> Result<TokenStream<'_, 'a>> {
        self.prompt.push_str(prompt);
        self.generate()
    }

    /// Generate more tokens without adding to the prompt
    pub fn generate(&mut self) -> Result<TokenStream<'_, 'a>> {
        debug!("Text prompt: {}", self.prompt);

        let n_past = self.ctx.get_kv_cache_used_cells() as i32;
        let is_first = n_past == 0;
        let add_bos = if is_first {
            AddBos::Always
        } else {
            AddBos::Never
        };

        // Tokenize the prompt
        let tokens_list = self
            .model
            .model
            .str_to_token(&self.prompt, add_bos)
            .map_err(|e| Error::TokenizationError(format!("failed to tokenize prompt: {}", e)))?;
        self.prompt.clear();

        let n_prompt = tokens_list.len() as i32;
        let n_ctx = self.ctx.n_ctx() as i32;

        info!(
            "n_prompt = {}, n_ctx = {}, n_past = {}",
            n_prompt, n_ctx, n_past
        );

        slide_cache(&mut self.ctx, n_prompt)?;

        // Add prompt tokens to batch
        let mut batch = LlamaBatch::get_one(&tokens_list).map_err(|e| {
            Error::BatchError(format!("Failed to create batch from prompt tokens: {}", e))
        })?;
        // FIXME: we decode the batch here because it incorrectly keeps a dangling pointer to token_list
        if let Err(e) = self.ctx.decode(&mut batch) {
            error!("llama_decode() failed: {}", e);
            return Err(Error::DecodingError(format!(
                "llama_decode() failed: {}",
                e
            )));
        }

        // Create and return the token stream
        Ok(TokenStream {
            session: self,
            n_decode: 0,
            batch: LlamaBatch::new(1, 1),
            is_complete: false,
        })
    }
}

fn slide_cache(ctx: &mut LlamaContext, n_tokens: i32) -> Result<()> {
    let n_ctx = ctx.n_ctx() as i32;
    let n_past = ctx.get_kv_cache_used_cells() as i32;
    if n_past + n_tokens > n_ctx {
        log::info!("KV cache is full, shifting context");
        let keep = n_ctx / 4; // tokens (at start) to keep
        let left = n_past - keep; // start index of tokens to discard
        let discard = left / 2; // number of tokens to discard
        if !ctx
            .clear_kv_cache_seq(
                Some(0),
                Some(keep as u32),
                Some(keep as u32 + discard as u32),
            )
            .unwrap()
        {
            return Err(Error::KVCacheSizeError(
                "failed to clear KV cache".to_string(),
            ));
        }
        ctx.kv_cache_seq_add(
            0,
            Some(keep as u32 + discard as u32),
            Some(n_past as u32),
            -discard as i32,
        )
        .unwrap();

        log::info!("cleared from {} to {}", keep, keep + discard);
    }

    Ok(())
}
