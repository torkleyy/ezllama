[package]
name = "ezllama"
version = "0.1.1"
edition = "2024"
description = "An opinionated, simple Rust interface for local LLMs, powered by llama-cpp-2"
license = "MIT"
repository = "https://github.com/torkleyy/ezllama"
documentation = "https://docs.rs/ezllama"
readme = "README.md"
keywords = ["llm", "ai", "llama", "language-model", "machine-learning"]
categories = ["api-bindings", "science", "text-processing"]
rust-version = "1.85.0"
include = ["src/**/*", "examples/**/*", "Cargo.toml", "LICENSE", "README.md"]

[dependencies]
llama-cpp-2 = { version = "0.1.102" }
log = "0.4.19"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0.140"
encoding_rs = "0.8"
tracing = "0.1"
lazy_static = "1.5.0"

[dev-dependencies]
tracing-subscriber = { version = "0.3", features = [
    "env-filter",
    "chrono",
    "local-time",
] }

[features]
metal = ["llama-cpp-2/metal"]
cuda = ["llama-cpp-2/cuda"]
vulkan = ["llama-cpp-2/vulkan"]
